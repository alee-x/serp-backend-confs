version: '3.7'

# Settings and configurations that are common for all containers
x-minio-common: &minio-common
  image: quay.io/minio/minio:RELEASE.2021-09-24T00-24-24Z
  command: server --console-address ":9001" http://minio{1...3}/data{1...2}
  expose:
    - "9000"
    - "9001"
  environment:
    MINIO_ROOT_USER: "${POSTGRES_USER}"
    MINIO_ROOT_PASSWORD: "${DB2INST1_PASSWORD}"
    MINIO_ACCESS_KEY: "${MINIO_ACCESS_KEY}"
    MINIO_SECRET_KEY: "${MINIO_SECRET_KEY}"
    MINIO_DOMAIN: "localmindomain.com"
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    interval: 30s
    timeout: 20s
    retries: 3

x-airflow-common:
  &airflow-common
  build: ./airflow-setup
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: amqp://${RMQ_U}:${RMQ_P}@rabbitmq/${RMQ_VH}
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    BROKER: rabbitmq
    CELERY_BROKER_USER: "${RMQ_U}"
    CELERY_BROKER_PASSWORD: "${RMQ_P}"
    CELERY_BROKER_HOST: rabbitmq
    CELERY_BROKER_PORT: 5672
  volumes:
    - ./airflow-setup/dags:/opt/airflow/dags
    - ./airflow-setup/logs:/opt/airflow/logs
    - ./airflow-setup/plugins:/opt/airflow/plugins
    - ./airflow-setup/tmp-ret:/tmp/great_expectations
  user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}"
  depends_on:
    rabbitmq:
      condition: service_healthy
    postgres:
      condition: service_healthy

x-resource-common:
  &resource-common
  deploy:
    resources:
      limits:
        cpus: 0.50
        memory: 256M

# starts 3 docker containers running minio server instances.
# using nginx reverse mproxy, load balancing, you can access
# it through port 9000.
services:
  minio1:
    <<: *minio-common
    <<: *resource-common
    hostname: minio1
    volumes:
      - data1-1:/data1
      - data1-2:/data2

  minio2:
    <<: *minio-common
    <<: *resource-common
    hostname: minio2
    volumes:
      - data2-1:/data1
      - data2-2:/data2

  minio3:
    <<: *minio-common
    <<: *resource-common
    hostname: minio3
    volumes:
      - data3-1:/data1
      - data3-2:/data2

  # minio4:
  #   <<: *minio-common
  #   <<: *resource-common
  #   hostname: minio4
  #   volumes:
  #     - data4-1:/data1
  #     - data4-2:/data2

  nginx:
    <<: *resource-common
    image: alleeex/serp-nginx:latest
    hostname: nginx
    ports:
      - "9000:9000"
      - "9001:9001"
    depends_on:
      - "minio1"
      - "minio2"
      - "minio3"
      # - "minio4"

  create_buckets:
    <<: *resource-common
    image: alleeex/serp-minio-mb:latest
    network_mode: host
    depends_on:
      - "minio1"
      - "minio2"
      - "minio3"
      # - "minio4"
    environment:
      MINIO_ROOT_USER: "${POSTGRES_USER}"
      MINIO_ROOT_PASSWORD: "${DB2INST1_PASSWORD}"
    tty: true

  rabbitmq:
    <<: *resource-common
    container_name: "rabbitmq"
    # image: rabbitmq:3.8-management-alpine
    image: alleeex/serp-rmq
    privileged: true
    environment:
        RABBITMQ_DEFAULT_USER: "${RMQ_U}"
        RABBITMQ_DEFAULT_PASS: "${RMQ_P}"
        RABBITMQ_DEFAULT_VHOST: "${RMQ_VH}"
    ports:
        # AMQP protocol port
        - '5672:5672'
        # HTTP management UI
        - '15672:15672'
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "5672"]
      interval: 5s
      timeout: 15s
      retries: 20
    restart: unless-stopped

  db2:
    image: alleeex/serp-db2:latest
    container_name: "db2"
    privileged: true
    ports:
      - "50000:50000"
      - "55000:55000"
    environment:
      DBNAME: "${DBNAME}"
      DB2INST1_PASSWORD: "${DB2INST1_PASSWORD}"
      LICENSE: "${LICENSE}"
      DB2INSTANCE: "${DB2INSTANCE}"
    volumes:
      - db2inst1:/database
    restart: unless-stopped

  mssql:
    image: alleeex/serp-mssql:latest
    container_name: "mssql"
    user: root
    environment:
      SA_PASSWORD: "${DB2INST1_PASSWORD}"
      ACCEPT_EULA: "Y"
    ports:
      - "5434:1433"
    volumes:
      - mssqlinst1:/var/opt/mssql
    restart: unless-stopped

  postgresfstore:
    <<: *resource-common
    image: alleeex/serp-pgresf:latest
    container_name: "postgresfstore"
    ports:
      - "5438:5432"
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${DB2INST1_PASSWORD}"
      POSTGRES_DB: "${DBNAME}"
    volumes:
      - postgresinst1:/var/lib/postgresql/data
    restart: unless-stopped

  nrdapi:
    image: alleeex/serp-nrdapi:latest
    container_name: "nrdapi"
    privileged: True
    ports:
      - "5000:5000"
    restart: unless-stopped

  postgres:
    <<: *resource-common
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: unless-stopped

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    container_name: airflow-webserver
    ports:
      - 8080:8080
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    restart: unless-stopped

  airflow-worker:
    <<: *airflow-common
    container_name: airflow-worker
    command: celery worker
    restart: unless-stopped

  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    command: version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
    depends_on:
      - "nginx"
      - "db2"
      - "postgresfstore"
      - "mssql"

  flower:
    <<: *airflow-common
    container_name: flower
    command: celery flower
    ports:
      - 5555:5555
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  spark-history:
    build: ./db-startup-scripts/spark/history
    container_name: spark-history
    privileged: true
    ports:
      - "18080:18080"
    depends_on:
      - spark-master
    volumes:
      - "./db-startup-scripts/spark/logs:/tmp/spark-events"
    restart: unless-stopped

  hadoop-submit:
    image: alleeex/serp-hadoop-submitter:latest
    container_name: hadoop-submitter
    depends_on:
      - datanode
      - namenode
      - hive-server
      - hive-metastore
      - hive-metastore-postgresql
    environment:
      NAMENODE_HOST: "namenode:9000"
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      ./db-startup-scripts/hadoop/hadoop.env

  hive-submit:
    image: alleeex/serp-hive-submitter:latest
    container_name: hive-submitter
    depends_on:
      - datanode
      - namenode
      - hive-server
      - hive-metastore
      - hive-metastore-postgresql     
    environment:
      NAMENODE_HOST: "namenode:9000"
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088 hive-server:10000 hive-server:10002 hive-metastore:9083 hive-metastore-postgresql:5432"
    env_file:
      ./db-startup-scripts/hadoop/hadoop-hive.env

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9900:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./db-startup-scripts/hadoop/hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    ports:
      - "9864:9864"
    env_file:
      - ./db-startup-scripts/hadoop/hadoop.env

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./db-startup-scripts/hadoop/hadoop.env

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./db-startup-scripts/hadoop/hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./db-startup-scripts/hadoop/hadoop.env

  spark-master:
    image: alleeex/serp-spark-master
    container_name: spark-master
    depends_on:
      - namenode
      - datanode
    ports:
      - "8070:8080"
      - "7077:7077"
    environment:
      INIT_DAEMON_STEP: "setup_spark"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:9000"
      SPARK_LOCAL_IP: "spark-master"
      SPARK_WORKLOAD: "master"

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    depends_on:
      - namenode
      - datanode
    env_file:
      - ./db-startup-scripts/hadoop/hadoop-hive.env
    command: /opt/hive/bin/hive --service hiveserver2 &
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"
      - "10002:10002"

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./db-startup-scripts/hadoop/hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864 hive-metastore-postgresql:5432"
    ports:
      - "9083:9083"

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql

  presto-coordinator:
    image: shawnzhu/prestodb:0.181
    container_name: presto-coordinator
    ports:
      - "8089:8089"


volumes:
  data1-1:
  data1-2:
  data2-1:
  data2-2:
  data3-1:
  data3-2:
  db2inst1:
  mssqlinst1:
  postgresinst1:
  postgres-db-volume:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver: